Great — I’ll research how to expand TransferAI from a v1 system (De Anza → UCSD, CS only) to v2 (De Anza → UCSD, all majors), with a strong emphasis on future-proofing the architecture for full CCC-to-UC statewide scaling. This will include scraper generalization, LLM system upgrades, articulation logic patterns, and best practices from similar high-trust LLM verticals.

I’ll let you know when your roadmap and architecture strategy are ready.

# TransferAI v2 Roadmap and Architecture for CCC–UC Articulation Advising

## Introduction  
TransferAI aims to be an intelligent academic advising assistant for community college students transferring to University of California campuses. To scale from a single major (De Anza College → UCSD Computer Science B.S.) to all majors and eventually all colleges, we need a robust strategy. This roadmap outlines: 

- The full spectrum of articulation **logic patterns** on ASSIST.org (e.g. “all courses required,” “select N of M courses,” cross-listed courses, petition requirements, etc.) and how to handle them.  
- How to **generalize the scraper and parser** to reliably capture these patterns in structured JSON.  
- A layered **LLM prompting architecture** to answer complex advising queries, including direct (UC course → CCC equivalent) and reverse (CCC course → UC credit) lookups, multi-step reasoning through requirements, and integration of general advising rules.  
- Evolution of the **JSON schema and prompting** to represent complex logic (like grouped options, partial articulations, “must complete at UC” flags) and to generate accurate, explanatory answers.  
- **Best practices** from high-trust vertical LLM applications (legal, medical, etc.), ensuring we combine strict rule reasoning with helpful natural language.  
- A phased **system roadmap** for v2 (De Anza → UCSD, all majors) and beyond (v3: all CCCs → all UCs), so the system scales confidently.

## Articulation Logic Patterns on ASSIST  
California’s ASSIST articulation agreements encode various requirement structures. TransferAI must recognize and handle all of these patterns in the data. Key articulation logic patterns include:

- **All Courses Required:** A list of courses where **every course is required**. The agreement may explicitly say “All of the following courses are required” or simply list courses without an “or”/“select” qualifier ([scrape.py](file://file-TJFmWV7Ppgit4ZzmSfwaK7#:~:text=return%20,choose_one_section)). For example, a major’s core might list several CCC courses each mapping to a UC course – and students must complete **all** of them. Section A of the Sociology articulation in ASSIST (see image below) shows three De Anza courses all required for the UCSD major’s core.  

- **“Select N Courses” Lists:** Requirements where students must **choose a certain number of courses from a list**. The text often reads “Select 1 course from the following” or “Select 2 of the following courses” ([2023-2024 General Education Requirements Agreement - ASSIST](https://www.assist.org/transfer/support?year=74&institution=132&agreement=114&agreementType=from&view=agreement&viewBy=breadth&viewSendingAgreements=false&viewByKey=74%2F114%2Fto%2F132%2FGeneralEducation%2F21ec3bad-8a12-407a-b843-89a413ee0729#:~:text=Students%20with%20partial%20IGETC%20certification,J)). The scraper should detect the number *N* and the list of options. In ASSIST HTML, these appear as an instruction header preceding a list of course options. For example, a sociology major might require “Select 1 course from the following” five options – meaning the student needs to take any one of those five courses. This corresponds to a logical **OR-group**, possibly with an explicit count (`select_n_courses`).  

- **Multiple Sections (Grouped Requirements):** Some majors break requirements into labeled sections (A, B, C, etc.) within a single group. Often **each section has its own rule**, and students must fulfill each section’s rule. For example, **Section A: All courses required** (core courses) and **Section B: Select 1 of the following electives**. In ASSIST, the group may not explicitly state the relationship between sections unless it’s an “or.” The default interpretation is that **all sections must be completed** (i.e. an AND between sections) unless otherwise noted. (If the group instruction says “Complete one of the following sections,” that would be a **choose-one-section** pattern – see below.) In our sociology example, Section A contains three required courses and Section B says “Select 1 course from the following” five courses. The student must complete all of Section A *and* one course from Section B to satisfy the group. 

    ([image]()) *Example articulation for a UCSD Sociology major (fictional layout for illustration). **Section A** lists De Anza courses (SOCI 1, 2, 60) mapping to UCSD courses (SOC 1, 20, 14) – all are required. **Section B** (instruction: “Select 1 course from the following”) lists five De Anza courses, of which only one (SOCI 20) has an articulated UCSD equivalent (SOC 5, “Same as INTL 8” cross-listed). The others show “No Course Articulated,” meaning those courses have no UCSD equivalent and thus the requirement would have to be fulfilled after transfer.* ([Agreement Report](https://www.sdccd.edu/docs/accreditation/mesa/Evidence%20Collection%20Form/Standard%20II/Evidence%20Upload%20-%20Standard%20II%20-%20A10/II.A.10-06_Articulation%20Agreement%20by%20Major%20%20.pdf#:~:text=BENG%201%20,after%20transfer%20END%20OF%20AGREEMENT)) ([Receiving Articulation | ASSIST API Documentation](https://prod.assistng.org/apidocs/docs/articulation/model/receiving#:~:text=course%20,attributes%20associated%20with%20the%20course))

- **Choose-One-Section (Either/Or Sections):** Occasionally, a requirement is divided into two or more alternative sections, and students need to complete **only one section or track**. The group instruction might say “Complete one of the following sections” or contain an “OR” indicating a choice between sections. For instance, an engineering major could allow *either* a physics sequence *or* a chemistry sequence to fulfill a science requirement. In JSON, this would be represented by a group with multiple sections and a logic flag like `choose_one_section`. The parser can infer this if the group’s instruction text contains keywords like “or” between section labels ([scrape.py](file://file-TJFmWV7Ppgit4ZzmSfwaK7#:~:text=return%20,choose_one_section)).

- **Cross-Listed Courses:** These are courses that exist under multiple department codes at the university. ASSIST displays a note like “Same as XXX” next to the course. For example, UCSD CSE 20 might be cross-listed as MATH 15A, meaning they are the same course for credit. The articulation will show “CSE 20 … **Same as MATH 15A**” on the UC side. The scraper should capture this detail so TransferAI can mention either code. It may be encoded in the ASSIST data as a “visibleCrossListedCourses” array ([Receiving Articulation | ASSIST API Documentation](https://prod.assistng.org/apidocs/docs/articulation/model/receiving#:~:text=course%20,attributes%20associated%20with%20the%20course)). Including cross-listed info in our JSON (e.g. an array of equivalent UC course codes or a note) will allow the LLM to recognize that, for instance, “CSE 20” and “MATH 15A” are interchangeable at UCSD.  

    ([image]()) *Example of a cross-listed course and OR-group on ASSIST. Here, **UCSD CSE 20 (Discrete Mathematics)** is noted as “Same as MATH 15A” (cross-listed). On the right, the De Anza equivalents are **MATH 22 or MATH 22H** (honors) – an OR-group where either course satisfies CSE 20. Cross-listed info (“Same as MATH 15A”) should be captured so that TransferAI can recognize both codes ([Receiving Articulation | ASSIST API Documentation](https://prod.assistng.org/apidocs/docs/articulation/model/receiving#:~:text=course%20,attributes%20associated%20with%20the%20course)), and the OR logic is captured as a list of options.* 

- **Combinations of Courses (Series / AND-groups):** Some **UC courses require a combination of multiple CCC courses** to fully satisfy the requirement, or vice versa. This often occurs with course sequences due to semester vs quarter system differences. For example, a year-long UC course sequence might map to two semester courses at the community college. ASSIST will show the CCC courses bracketed together or joined by “AND.” A student must take **all courses in the set** to receive credit for the one (or more) UC course(s). Our parser already handles this by detecting bracketed course groups and marking them as `type: "AND"`. For instance, UCSD’s Organic Chemistry II (CHEM 41B) might require **two** Mesa College courses – CHEM 231 *AND* CHEM 233 – to articulate fully ([Agreement Report](https://www.sdccd.edu/docs/accreditation/mesa/Evidence%20Collection%20Form/Standard%20II/Evidence%20Upload%20-%20Standard%20II%20-%20A10/II.A.10-06_Articulation%20Agreement%20by%20Major%20%20.pdf#:~:text=CHEM%2041A%20,after%20transfer%20END%20OF%20AGREEMENT)). If only one is taken, it’s a **partial articulation** – not enough to satisfy the requirement. TransferAI should be able to explain these cases (e.g. “you must take both Course X and Y at De Anza to get credit for UCSD’s Z ([Agreement Report](https://www.sdccd.edu/docs/accreditation/mesa/Evidence%20Collection%20Form/Standard%20II/Evidence%20Upload%20-%20Standard%20II%20-%20A10/II.A.10-06_Articulation%20Agreement%20by%20Major%20%20.pdf#:~:text=CHEM%2041A%20,after%20transfer%20END%20OF%20AGREEMENT))”). Partial articulations also appear as “*No course articulated*” if a sequence is incomplete – for example, if a student took one course of a required pair, ASSIST might not award any credit until both are completed.

- **“No Course Articulated” and **Must Complete at University**:** “No Course Articulated” appears when the community college offers a course (or requirement) that has **no equivalent at the UC campus**. In ASSIST, this is literally displayed in the CCC side as a placeholder. It signifies that the student **cannot fulfill that requirement at the community college**. Sometimes the agreement explicitly adds a note like “This course must be taken at the university after transfer” for clarity ([Agreement Report](https://www.sdccd.edu/docs/accreditation/mesa/Evidence%20Collection%20Form/Standard%20II/Evidence%20Upload%20-%20Standard%20II%20-%20A10/II.A.10-06_Articulation%20Agreement%20by%20Major%20%20.pdf#:~:text=BENG%201%20,after%20transfer%20END%20OF%20AGREEMENT)). TransferAI should detect when a requirement has no CCC option and advise the student that they will need to take that course at the UC (and perhaps why – e.g. it’s an upper-division or unique course). These often are capstone or seminar courses or very new courses with no CC equivalent. In our JSON, we already mark `no_articulation: true` for these cases. We can enhance this by capturing any accompanying text (for instance, an agreement might list “BENG 1 – Introduction to Bioengineering – *This course must be taken at the university after transfer*” ([Agreement Report](https://www.sdccd.edu/docs/accreditation/mesa/Evidence%20Collection%20Form/Standard%20II/Evidence%20Upload%20-%20Standard%20II%20-%20A10/II.A.10-06_Articulation%20Agreement%20by%20Major%20%20.pdf#:~:text=BENG%201%20,after%20transfer%20END%20OF%20AGREEMENT))).

- **Petition-Only Articulations:** Some courses are not pre-approved for articulation, but students can get credit via a **petition after transfer**. ASSIST flags these with notes like “Petition department after transfer” ([Agreement Report](https://www.sdccd.edu/docs/accreditation/mesa/Evidence%20Collection%20Form/Standard%20II/Evidence%20Upload%20-%20Standard%20II%20-%20A10/II.A.10-06_Articulation%20Agreement%20by%20Major%20%20.pdf#:~:text=Petition%20department%20after%20transfer%20%E2%86%90,Electricity)). For example, UCSD might not officially articulate a special topics course, but a student can petition to have it count toward a requirement. The UI may show a bracket or note on the UC side. In our data, these currently appear as “No Course Articulated” (since there’s no direct articulation), but with an advisory note. We should capture the “petition” flag or text. TransferAI can then alert the student: e.g. *“Course X at De Anza might count for UCSD credit upon petition, but isn’t automatically accepted ([Agreement Report](https://www.sdccd.edu/docs/accreditation/mesa/Evidence%20Collection%20Form/Standard%20II/Evidence%20Upload%20-%20Standard%20II%20-%20A10/II.A.10-06_Articulation%20Agreement%20by%20Major%20%20.pdf#:~:text=Petition%20department%20after%20transfer%20%E2%86%90,Electricity)).”*  

    ([image]()) *Example of a petition-only articulation. UCSD **MATH 20E (Vector Calculus)** has no direct De Anza equivalent, but ASSIST shows a note “Petition department after transfer.” This means a student who took a comparable course (perhaps De Anza’s MATH D003?) must petition UCSD’s Math department to receive credit. TransferAI should flag such cases clearly (neither fully articulated nor impossible, but requiring additional approval) ([Agreement Report](https://www.sdccd.edu/docs/accreditation/mesa/Evidence%20Collection%20Form/Standard%20II/Evidence%20Upload%20-%20Standard%20II%20-%20A10/II.A.10-06_Articulation%20Agreement%20by%20Major%20%20.pdf#:~:text=Petition%20department%20after%20transfer%20%E2%86%90,Electricity)).* 

- **General Electives / Unit Credit:** Some agreements include an “Electives” or additional unit requirement, e.g. “Any UC-transferable course in ___” or “__ units of electives from approved list.” These are less common in major articulation (more common in general education or minor requirements), but if present, the logic is that **many courses could fulfill the requirement**. For instance, a major might accept “any **transferable** course in the biological sciences” for an elective slot. TransferAI should recognize such open-ended requirements from the instruction text (likely captured in the group’s or section’s instruction string). In these cases, the JSON might not list specific course mappings at all – just an instruction. We should preserve that instruction and perhaps tag the requirement type as “elective” or “free choice.” The advising content would then rely on general rules (e.g. identifying if a course is UC-transferable) to determine if a student’s course could count. A general rule here is: if ASSIST doesn’t list a specific articulation, a course might still transfer for unit credit (as long as it’s UC-transferable, which is determined by the UC’s transfer criteria). We will incorporate this knowledge so the LLM can handle questions like “Will course X (that’s not articulated) still give me credit?” by explaining it counts for units but not for that specific requirement unless petitioned.

- **Shared or Overlapping Requirements:** When scaling to multiple majors and campuses, we’ll notice some **shared articulation patterns**. For example, many engineering majors share the same math/physics requirements. TransferAI’s knowledge base might hold duplicate copies of these articulations per major, but logically they are the same rule. In the future, we can optimize by recognizing shared groups (perhaps by identical instruction text and course lists) so we only need to parse logic once. For now in v2, we simply ensure each major’s JSON is complete, but be aware that “shared” means if one articulation data updates, others might too. There may also be cases where one community college course satisfies a requirement in multiple majors – the system should be able to reuse that info when advising across different majors (this is more about data reuse than an ASSIST pattern, but it’s a consideration for scaling).

Understanding these patterns is crucial for both the **parser** (to correctly structure the data) and the **LLM** (to reason about satisfaction of requirements). Next, we address how to adapt our scraper/parser to capture all these nuances.

## Generalizing the Scraper & Parser for All Patterns  
Our current Selenium scraper and parser (used for the one CS major) must be extended to handle the above patterns across ~150 majors for De Anza→UCSD. Key improvements include:

- **Robust Retrieval of All Groups/Sections:** Ensure the scraper scrolls and loads all **groupContainers** and **emphasis--section** elements on the ASSIST page. The `scroll_all_emphases()` function in v1 already attempts this. For v2, we should verify it works for every major (some majors have many groups/sections). If any sections are hidden behind expandable menus or tabs (for instance, some majors have “Emphases” or specializations), the scraper must click those or otherwise retrieve that content. We should also capture the **major title and year** (which we already do: `major`, `from`, `to`, `catalog_year` in JSON).

- **Capture Group and Section Instructions Text:** The parser currently uses `get_group_instructions()` to grab any instruction text for a group (e.g. “Select 2 courses from the following”). We should double-check this function against various patterns:
  - If the instruction is not in a straightforward place, try multiple selectors (the code already checks `.awc-instruction-section`, `.instructionsHeader`, `.instructionsPreview`, etc.). Some agreements put instructions in different spans or as part of the first section’s header. We should verify that phrases like “Select 1 course from the following” or “Complete one of the following sections” are captured. This text is key to determining the logic.
  - Section-level instructions: In our JSON design, each section also has an `instructions` field (combining section label and header). For example, section “B” might have `instructions: "Select 1 course from the following."` captured. The current parser does capture section headers (like letter labels and any embedded text) ([scrape.py](file://file-TJFmWV7Ppgit4ZzmSfwaK7#:~:text=section_id%20%3D%20section_label%20if%20section_label,section_header%20else%20section_label%20or%20section_header)). We should ensure it grabs things like “Select 1 course from the following” when they apply to a section (perhaps the site attaches that text as a section header in some cases).

- **Identifying Pattern Keywords:** We can refine `infer_group_logic_type()` (and possibly create an `infer_section_logic_type`) using keywords:
  - “all” and “required” → all_required (unless overridden by more specific conditions).
  - “select”/“choose” + number → select_n_courses (and use regex to pull the number ([scrape.py](file://file-TJFmWV7Ppgit4ZzmSfwaK7#:~:text=def%20infer_required_course_count%28instruction%3A%20str%29%20,group%281%29%29%20if%20match%20else%201))).
  - “Complete one of the following sections” or presence of “OR” between section labels → choose_one_section ([scrape.py](file://file-TJFmWV7Ppgit4ZzmSfwaK7#:~:text=return%20,choose_one_section)).
  - If none of these and multiple sections exist, assume all sections required (with their internal rules).  
  This inference helps populate a field in JSON (like `group_logic_type`) and can also be used in prompts, but we should store the raw instruction text as well for fidelity.

- **Parsing Course Mappings (UC→CCC):** The existing logic iterates through each **articulation row** (`articRow` elements) and splits into `rowReceiving` (UC side) and `rowSending` (CCC side) ([scrape.py](file://file-TJFmWV7Ppgit4ZzmSfwaK7#:~:text=for%20rowset%20in%20row_sets%3A%20try%3A,text.strip%28%29%20units%20%3D%20None)) ([scrape.py](file://file-TJFmWV7Ppgit4ZzmSfwaK7#:~:text=ccc_block%20%3D%20rowset.find_element%28By.CLASS_NAME%2C%20,orGroup)). This should largely remain the same for all majors:
  - Extract UC course code, title, and units (if present). One extension: **capture UC course attributes** if present. For example, if the UC course has a cross-listed code or a footnote like “(Credit limit: 2 units)” or asterisks, we might capture those as part of title or a separate attribute list.
  - On the CCC side, gather the equivalent courses. The parser already handles two scenarios:
    1. Multiple OR options (multiple `.orGroup` elements): it calls `parse_equivalent_sets_from_sending_block()` which builds an OR list ([scrape.py](file://file-TJFmWV7Ppgit4ZzmSfwaK7#:~:text=return%20%7B%20,course_paths)).
    2. Within each OR option, if there’s a bracket grouping multiple courses (the code checks for `.bracketWrapper`), it collects them as an AND-group ([scrape.py](file://file-TJFmWV7Ppgit4ZzmSfwaK7#:~:text=elif%20,AND)).  
    We should test this thoroughly with known cases: e.g. UCSD Chemistry, Physics sequences, etc., to ensure that when multiple CCC courses are required for one UC course, our JSON outputs `{ type: "AND", courses: [ ... ] }` inside the OR list. The snippet from a UCSD–Mesa agreement confirms this format: UC CHEM 41B was mapped to CHEM 231 *AND* CHEM 233 ([Agreement Report](https://www.sdccd.edu/docs/accreditation/mesa/Evidence%20Collection%20Form/Standard%20II/Evidence%20Upload%20-%20Standard%20II%20-%20A10/II.A.10-06_Articulation%20Agreement%20by%20Major%20%20.pdf#:~:text=CHEM%2041A%20,after%20transfer%20END%20OF%20AGREEMENT)).
  - If no courses are articulated (no `.courseLine` elements on the CCC side), our function returns a `no_articulation_block()` which currently creates a dummy OR with “No Course Articulated” ([scrape.py](file://file-TJFmWV7Ppgit4ZzmSfwaK7#:~:text=def%20no_articulation_block%28%29%3A%20return%20%7B%20,)). We might enhance this to mark it distinctly, e.g. `no_articulation: true` (already done) and possibly attach any note found (like “must be taken at UC”). Often, the presence of `no_articulation` implies exactly that, but we saw cases with explicit phrasing. The HTML might include that phrasing in a span (the PDF suggests it’s just textual). We could search within the `rowSending` element for any text that isn’t in a `.courseLine` – perhaps ASSIST inserts a `<i>` or `<span>` with the advisory. Our `get_group_instructions` tries `.sectionAdvisements`, which might catch something at section level. It might be safer to also check at the row level: e.g., if `ccc_block.text` contains “Petition” or “university after transfer,” capture that as a note.

- **Capturing Attributes and Notes:** Beyond courses, articulation data can have **attributes** (as the ASSIST API calls them) for specific rows. Examples:
  - “Petition department after transfer” – likely an attribute on the UC course side or a note on the CCC side. We should scrape these by looking for any element with a class indicating notes or footnotes. In the ASSIST site, such notes might appear as bullet icons with hover text, or inline italic text. Since we saw them in a PDF, in the web interface they could be present as text in the row (possibly with a specific CSS class or as part of the UC course cell).
  - Cross-listed “Same as ___” – in the UI, this appears under the UC course title, with a gray background. It might be a `<div>` or `<span>` with text “Same as XYZ”. We should find it via Selenium. For example, after getting `uc_block`, we could attempt something like: `uc_block.find_element(By.XPATH, ".//*[contains(text(), 'Same as')]")` to see if such text exists, and parse out the cross-listed code. If found, add it to the UC course info (e.g., `uc_course_alt_codes: ["MATH 15A"]`). If not found easily via text, maybe check for a specific class in that element (the screenshot shows it as a gray bubble; possibly it’s rendered as a small `<span class="courseNote">` or similar).
  - Other possible attributes: “Credit limitation”, “No credit if taken after ___”, etc. These might appear as notes too. While not explicitly mentioned in our scope, if we encounter them, capturing them would improve advising accuracy (e.g., warn a student if a course overlaps credit with another).

- **Normalization and Consistency:** The parser’s `normalize()` function already formats each course entry with a consistent `course_id` (hash) and separates the course prefix+number vs title ([scrape.py](file://file-TJFmWV7Ppgit4ZzmSfwaK7#:~:text=def%20normalize,False)). As we scale to many majors, consistent identifiers will be important. We should ensure the `course_id` is unique per course code (the current approach MD5-hashes the course “letters” like “MATH 22” to 8 chars). This helps if we want to cross-reference courses across majors or create reverse lookup indices. We might consider including the institution in the hash to avoid collisions (though likely not needed since CCC and UC codes won’t coincide, but just in case e.g. “SOC 1” at UC vs “SOC 1” at CCC, they are different courses – a combined key could be “UCSD SOC 1” vs “De Anza SOCI 1”).

- **Testing Diverse Majors:** Once the parser is updated, run it on a variety of majors (STEM, humanities, etc.) to catch edge cases:
  - Majors with **no articulation at all** for some courses (should yield “No Course Articulated” entries).
  - Majors with multiple alternative tracks (ensure the group instruction “choose one section” is captured).
  - Majors where the CCC offers more courses than needed (like “select 2 of 5”) vs cases where CCC offers fewer courses than the UC requires (which results in no articulation or partial sequences).
  - Cross-listed departments (e.g., International studies, interdisciplinary majors).
  - Ensure performance: with ~150 majors, the scraper will be doing a lot of Selenium navigation. We might optimize by reusing the browser session for all majors (select major -> scrape -> go back -> next major), rather than restarting each time. Introduce short delays or waits as needed to avoid missing dynamic content load.

- **JSON Schema Extensions:** After parsing, we will likely augment the JSON structure for v2:
  - Add a top-level field for `major` (already there) and possibly `major_category` if we parse (not crucial, but maybe if the UI provides any grouping of majors).
  - Within each group, include a derived `group_logic_type` (as we do) and perhaps a `sections_relation` field (e.g., `"sections_relation": "AND"` or `"OR"`) to explicitly indicate how sections combine. This can be inferred by `group_logic_type` (all_required = AND between sections, choose_one_section = OR between sections).
  - Each section could have a `section_logic_type` if needed (though often it's just all courses in section, except if section itself has an internal “select N” which is indicated by its instruction text).
  - For each course mapping entry:
    - Possibly split into a structure like `{ "uc_course": {...}, "equivalents": {...} }`. Currently we store `uc_course` as a string like `"CSE 20: Discrete Mathematics"`, and `equivalent_sets` as the CCC side structure. We might instead want to store more details under `uc_course` (like code, title, units, attributes) for easier processing. Similarly, the CCC side could be structured. But since we mostly need text for advising, the current format is workable. We can also keep it simple and annotate where needed.
    - Add an `attributes` list for each mapping if we captured any (e.g., `[ "Petition after transfer", "Must take at UCSD", "Cross-listed as INTL 8" ]`). These could also be broken into boolean flags like `petition_required: true`, but a list of strings might be easier to just output in natural language. The key is the LLM can read these and incorporate them into its answer.
    - Mark honors courses distinctly (the parser sets an `honors` flag on course names containing "HONORS"). We should ensure honors versions are recognized as equivalent choices (e.g., MATH 22 vs 22H are interchangeable in OR). This appears to be handled by listing both in the OR.
  - **Elective or generic rules** might have no specific course entries. In such cases, we could represent them as a pseudo-course like `{ "uc_course": "Unspecified elective", "equivalent_sets": { "type": "ANY", "criteria": "Any transferable course in X" } }` or simply handle it in instructions text. Possibly easier: if a group has no course list, just an instruction, we handle it purely in text.

By generalizing the scraper/parser as above, we will obtain a comprehensive, consistent JSON for all De Anza→UCSD articulations. This provides the knowledge base for TransferAI. 

The next step is designing the LLM component to use this data effectively – answering students’ questions, reasoning through the logic, and providing clear guidance.

## LLM Prompting Architecture and Reasoning Strategy  
Expanding to all majors and complex requirements means our LLM prompts and workflow must be sophisticated. We propose a **multi-layer prompting architecture** combined with rule-based checks to ensure accurate and context-aware answers. Key design points:

### Multi-Layer Prompting Strategy  
Rather than always dumping the entire major’s data into a prompt, we use a layered approach that provides the **appropriate scope of information** based on the query. This makes reasoning more tractable and keeps the LLM focused:

- **Course-Level (Micro) Prompting:** If the query is very specific – e.g., “Does **De Anza MATH 22** satisfy any requirement for **UCSD**?” or “What is **UCSD CSE 20** equivalent at De Anza?” – we can narrow context to the single relevant mapping. The system can search the JSON for the specific course code (CCC or UC) and extract the corresponding row or snippet. The prompt to the LLM would include just that mapping’s details (e.g., “UCSD CSE 20 = De Anza MATH 22 or MATH 22H; note: CSE 20 is same as MATH 15A at UCSD, 4 units”). With that, the LLM can directly answer something like: *“Yes, CSE 20 at UCSD can be taken at De Anza as MATH 22 (or the honors MATH 22H) ([image]()). They are considered equivalent.”* This micro-level Q&A ensures precision for direct articulation queries. We will also handle the inverse: if the student provides a CCC course, we find what UC course it articulates to (if any) and present that.

- **Section-Level Prompting:** For questions that involve a *set* of courses or a category of requirement. For example, “I took **SOCI 10 and SOCI 20** at De Anza – did I meet the sociology elective requirement?” This pertains to Section B in the earlier example. The system would identify that SOCI 10 and 20 correspond to that section’s courses. It would then provide the LLM context on Section B: the instruction (“Select 1 of the following courses”) and the list of options with their articulation status (SOCI 10 – no articulation; SOCI 20 – articulates to SOC 5, etc.). The LLM can then reason: the requirement needs one articulated course; the student took SOCI 20 which *is* articulated to SOC 5, so they fulfilled it (if so). Or if the student took SOCI 10, the LLM would see SOCI 10 maps to “No Course Articulated” and conclude the requirement is not fulfilled (they took a course that doesn’t count for the UC requirement). By focusing on a single section’s data, the prompt stays concise but allows the LLM to apply the requirement logic to the student’s courses.

- **Group-Level Prompting:** If a question spans an entire requirement group or multiple requirements – e.g., “What courses do I need to complete **all lower-division requirements** for this major at De Anza?” – we’ll need to consider the whole group (or multiple groups). For instance, “What are the complete set of courses I should take at De Anza for UCSD Computer Science lower division?” covers math, physics, programming, etc. In this case, we’d include the relevant groups (often labeled by number) from the JSON in the prompt. We might feed the LLM the instructions and a summary of each section and its course mappings. The LLM can then enumerate what the student should take: e.g. “You need all courses in Group 1 (Calculus I, II, III, Discrete Math, etc.) and one course from Group 2 (either X or Y),” based on reading the group’s content. Essentially, the LLM turns the structured data into an **advising plan**. We must ensure the model doesn’t hallucinate courses – by giving it the actual list, we anchor its output to real data.

- **Major-Wide or Multi-Group (Macro) Prompting:** In cases like “Summarize all the articulation agreement details for X major” or “Have I met all requirements for transfer?” which could involve checking every group in a major or even multiple majors, we might have to feed larger portions or do iterative prompting. One approach is to tackle each group with the LLM separately (asking it to evaluate fulfillment or list needed courses per group), then combine results for a final answer. Another approach is retrieval-based: use a vector index where each group’s text is a document, retrieve relevant ones, and let the LLM handle them together if small enough. This ensures scalability when v3 covers an entire CCC → multiple UCs: we won’t load everything, just what’s needed.

- **Fallback and General Knowledge:** If the user asks something outside the scope of specific articulation (like “What is ASSIST?” or “What does ‘No Course Articulated’ mean?”), the system should have a fallback. This could be a prompt that uses the **general advice** text we scraped (often agreements have a general note about GPA or IGETC, etc.) or rely on the LLM’s trained knowledge. For example, we captured any “generalInformation” from the page (which might say things like “Partial IGETC not accepted for this major,” etc.). The LLM can incorporate that for general questions like “Do I need IGETC for UCSD?” if it was in the general notes. For broader questions on transfer policy (not specific to a major), the LLM’s base knowledge might suffice, but since this is a high-trust domain, we should verify answers. We can maintain an internal FAQ or knowledge base for such common questions to either directly answer or augment the LLM’s response.

### Direct vs. Reverse Query Handling  
We need to handle both **forward articulation** (UC requirement → what CCC courses fulfill it) and **reverse articulation** (CCC course → does it fulfill anything at the UC):

- **Forward (UC → CCC):** This is the natural direction of our data: given a UCSD course or requirement, list the De Anza equivalent(s). Most advising questions, like “What should I take at De Anza for UCSD’s X course?” or “Does De Anza offer an equivalent to UCSD Y?” fall in this category. Our JSON is structured by UC course within sections, so it’s straightforward to find the UC course and read off the CCC options. The LLM’s job is to translate that into advice (including any caveats like petition or needing multiple courses). For example, if asked “How can I satisfy UCSD’s ENG 100 requirement at De Anza?”, the system finds that UCSD ENG 100 is articulated to, say, De Anza EWRT 1A + EWRT 1B (just an example). It then prompts the LLM with: “UCSD ENG 100: Equivalent at De Anza = EWRT 1A AND EWRT 1B.” The LLM would answer: *“To get credit for UCSD’s ENG 100, you need to take **both** EWRT 1A and EWRT 1B at De Anza (the combination of those two corresponds to ENG 100).”* 

- **Reverse (CCC → UC):** These are queries like “I took De Anza **SOCI 20** – what does that count for at UCSD?” or “Is De Anza’s **MATH D003** useful for my UCSD major?” For this, we can build a **reverse index** from the JSON. Upon data ingestion, for each CCC course in an equivalent set, map it to the UC course (and the requirement context) it satisfies. For example, map “De Anza SOCI 20” → “UCSD SOC 5 in Sociology major Group 1 Section B”. This could be a simple dictionary where key is CCC course code and value is a list of (UC course, major, group/section) tuples. We must be careful: the same CCC course could articulate to different UCs or different majors’ requirements if those majors have overlapping needs. In v2 (one CCC->one UC), it likely articulates to the same UC course across majors if the major requirements overlap (and indeed, ASSIST agreements for different majors might repeat the same mapping). We should handle the scenario: if a student asks about a CCC course without specifying major, do we list all majors it’s useful for? That might be beyond scope – likely the context will be within a major or a specific target UC. We can clarify if needed (“for which UC or major?”). Initially, we can assume questions are within the context of a chosen major or UC campus, since TransferAI can prompt the user or know which agreement they’re interested in. 

  Implementation-wise, performing a reverse lookup is as simple as searching our JSON structures. If we don’t precompute a dictionary, we can search through all `equivalent_sets` for a matching course name. Since performance on one JSON (a major) is fine, we could do that on the fly. But to scale up (all majors, all colleges), an indexed approach is better. For v2, either method is okay. For example, user asks “Does De Anza MATH 1A count for anything at UCSD?” – we scan UCSD’s agreements for “MATH 1A” and find maybe it maps to UCSD Math 20A in multiple majors (Math major, Engineering majors, etc.). We could then present the answer specific to the relevant major or generally “Yes, MATH 1A at De Anza is UCSD’s Math 20A (Calculus I) ([Agreement Report](https://www.sdccd.edu/docs/accreditation/mesa/Evidence%20Collection%20Form/Standard%20II/Evidence%20Upload%20-%20Standard%20II%20-%20A10/II.A.10-06_Articulation%20Agreement%20by%20Major%20%20.pdf#:~:text=CHEM%2041A%20,after%20transfer%20END%20OF%20AGREEMENT)), which is required for many STEM majors.”

- **Handling Non-Articulated CCC Courses:** If reverse lookup finds nothing (e.g., a student took a course not listed in any articulation for that campus), TransferAI should inform them that the course likely transfers as elective credit only, not towards the major requirements. This is where **general rules** come in: any transferable course can still count for unit credit at UC, but if ASSIST shows no equivalency, it won’t satisfy a specific requirement without further action. The advisor output might be: “De Anza’s ____ is not listed as equivalent to any required course at UCSD – you might still get general credit for it, but it won’t fulfill a major requirement. You may need to petition if you think it covers similar content.”

In summary, the system will use the direction of the query to choose how to traverse the data and assemble the prompt for the LLM. 

### Integrating General Advising Rules and Domain Knowledge  
Beyond the raw articulation mappings, a good advising system must incorporate **policies and heuristics** that human advisors use. We will encode some of these in either the data or the prompt:

- **UC-Transferability Rule:** If a course is UC-transferable (generally, all courses listed on ASSIST are, but there are courses at CCC that are not UC-transferable at all), then it will count for at least elective credit at the UC. We can have the LLM mention this for courses that have no articulation: *“Course X will still transfer for credit (since it’s transferable), but…”* If the user specifically asks about transferability, we might need a source. For now, we assume courses in these agreements are transferable; if the user asks about a course not in the agreement, we might need to consult the college’s transferability list. (In future, we could integrate ASSIST’s “Transferability” reports or a list of UC transferable courses by college).

- **Grade Requirements:** Many UCs require a minimum grade (usually C or better) for transfer courses to count. ASSIST doesn’t always state this, but general policy does. We might include a general note in our system (like “Remember you need a C or better for it to count”). The “generalInformation” scraped might include something about GPA or grade if provided. If not, we can incorporate this as a static rule in the LLM prompt or as part of answer templates.

- **Credit Limits or Restrictions:** Occasionally, there are rules like “Students will only receive credit for one of [Course X or Y]” or “Maximum 70 transfer units will count toward the degree” or “Lower division courses in the major must total to ...”. While not directly an articulation issue, these affect advising. We should decide if TransferAI will handle such general counseling points. To keep v2 focused, we might limit to the articulation itself, but it’s wise to acknowledge if a question touches on these (e.g., if asked “Can I take extra courses?” or “Will I get credit for both X and Y?” and we know they are articulated to the same UC course, the system can respond that UCSD will only count one). This likely involves having some curated knowledge or recognizing overlapping articulations.

- **Partial IGETC or GE patterns:** Sometimes ASSIST major agreements note general education recommendations (like “IGETC not accepted” or “Partial IGETC allowed except for ...”). Our system could read those from general info and answer questions about GE in the major context. This is tangential to major articulation, but could be included. Likely, incorporate any text from the agreement’s notes, e.g. *“UC Davis accepts partial IGETC”* ([Optimize Your Transfer to UC Davis: Articulation Agreement and ...](https://www.coursesidekick.com/english/13423038#:~:text=Optimize%20Your%20Transfer%20to%20UC,Organic%20Chemistry)) if such appears for a UCSD major, and have the LLM mention it if asked.

- **Petition and Exam Policies:** As identified, if something is marked petition or placement exam, the LLM should know to advise the student to follow that process (e.g., “you will need to pass the UCSD placement test to get credit for this” if “subject to placement by proficiency exam” was noted ([Agreement Report - San Diego Community College District](https://www.sdccd.edu/docs/accreditation/mesa/Evidence%20Collection%20Form/Standard%20II/Evidence%20Upload%20-%20Standard%20II%20-%20A10/II.A.10-06_Articulation%20Agreement%20by%20Major%20%20.pdf#:~:text=%E2%80%8BArticulation%20is%20subject%20to%20placement,Mechanics%20%284.00))). We can hardcode some explanation of common phrases: 
  - “Petition after transfer” means the student should keep syllabus/records and file a petition at UCSD’s department; 
  - “placement by exam” means even with CC course credit, UCSD requires a placement test (common in language or math sequences).
  The LLM can be instructed via few-shot or by including those phrases in context with an explanation.

By merging these general rules with the specific JSON data, TransferAI will not only parrot the articulation info, but also provide **advising insight** (much like a human counselor saying “This class won’t count for the major, but you’ll get units – if you really want it to count, you’d have to petition.”).

### Logical Validation of Student Progress  
A critical use-case is validating whether a student’s completed courses fulfill the requirements. This is essentially a **constraint satisfaction check**:
- For each requirement group or section: have they taken the required number of courses? If a course is required, did they take it (or its equivalent)? If they had options, did they take an acceptable one (and not double-count where not allowed)? 

We will likely implement a function on the JSON to do this check systematically, but the LLM can also assist in explaining the result. The plan:
1. **Identify the student’s courses** in terms of our data. For example, map “I took CIS 22B and MATH 1B at De Anza” into course identifiers that appear in the JSON (CIS 22B, MATH 1B etc.). We may need a normalization (our JSON might call it “CIS 22B” exactly if that’s how it appears).
2. For each requirement group of the major, determine which parts are satisfied:
   - If group is all-required (and not multiple sections), then every UC course listed should have an equivalent that the student took. 
   - If group has sections A, B, etc.:
     * Did the student complete all of Section A’s courses (or their CCC equivalents)?
     * Did they fulfill Section B’s rule (took at least N of the courses in that list)?
     * And so on.
   - If group is a choose-one-section type: check each section and see if one section is fully satisfied; if yes, the group is satisfied.
   - If a required course has an AND (multiple CCC needed), ensure the student took all in that AND set.
3. The result of this check could be a structured summary, like:
   - Group 1: 2/3 courses completed; missing “Physics 2C” (no CC equivalent, must take at UC).
   - Group 2: 1/1 section chosen (Section B completed via SOCI 20).
   - etc.
   We can generate such summary with code, since it’s straightforward logic.

We would then use the LLM to turn this into a friendly explanation for the student:
   - e.g. “You have completed most requirements, but you still need to take **UCSD’s PHYS 2C** (or its equivalent). De Anza doesn’t offer an equivalent for PHYS 2C ([Agreement Report](https://www.sdccd.edu/docs/accreditation/mesa/Evidence%20Collection%20Form/Standard%20II/Evidence%20Upload%20-%20Standard%20II%20-%20A10/II.A.10-06_Articulation%20Agreement%20by%20Major%20%20.pdf#:~:text=Petition%20department%20after%20transfer%20%E2%86%90,Electricity)), so you’ll have to take that after transfer. Also, for the programming requirement, you need 2 courses; you’ve taken only one, so one more is required from that list.” 
This blends facts from the check with natural reasoning. 

By validating via code or at least by systematic logic before the final answer, we ensure the LLM doesn’t incorrectly assume a requirement is met or not. The LLM will be given a “checklist result” and asked to elaborate. Alternatively, we could feed all the student’s courses + all requirements into the LLM and let it reason stepwise, but that is riskier. A hybrid approach is safer: we do the heavy lifting in code and let the LLM do the explanation and user-friendly phrasing.

### Ensuring Prompt Accuracy and Controlling Output  
In a high-stakes domain (students’ academic plans), we want to minimize hallucinations or errors. Some best practices we’ll adopt in the prompting and architecture:

- **Use the JSON as Ground Truth:** Wherever possible, the LLM’s knowledge should come from the JSON data (which comes from ASSIST). We will incorporate the relevant JSON excerpts in the prompt so the model isn’t recalling outdated info. The prompt might say (in system or few-shot examples): *“Base your answer only on the provided articulation data.”* If we have any doubt, we can also ask the model to quote the requirement (though maybe not necessary to show the user, but to ensure fidelity).
- **Few-Shot Examples:** We can include a couple of QA examples in the prompt, demonstrating how to handle tricky cases – e.g. one example of answering a direct articulation, one of explaining a partial (no articulation) scenario. This can guide the model’s style (e.g., always mention if something must be taken at UC, etc.).
- **Chain-of-Thought for Complex Reasoning:** We could instruct the model to think step by step (not necessarily show to user, but in prompt as hidden chain-of-thought) for multi-step questions. However, since we plan to do logic via code mostly, we may not need the model to do explicit chain-of-thought on counting courses. But for completeness, if we had to, we could prompt it like: “First, identify the requirement. Second, check which options the student took…” etc. This might improve correctness but can be tricky to manage reliably.
- **Verification and Debugging:** We should test the LLM outputs against known scenarios. For example, pose a scenario where we know the answer (like a student completed everything except one course) and see if the LLM correctly identifies the missing piece. If not, adjust the prompt or add clarity (like explicitly listing what’s missing for it to include).
- **Tone and Clarity:** Prompt the LLM to be *advisory* and factual. Possibly instruct it to cite course codes exactly as in data, and to be clear about any uncertainty (e.g., “if you take X, it should count, but confirm with an advisor for petition cases”). This is important in high-trust advice: acknowledging uncertainty where it exists is better than false certainty.

With the prompting strategy in place, we will also update the **JSON schema and how we feed it** to the LLM for maximum effectiveness, which we cover next.

## Evolving JSON Schema and Prompting for Complex Logic  
To support the complex logic and ensure consistent answers, we’ll refine both the JSON format and how we construct prompts from it:

- **JSON Schema Updates:**  
  Our current JSON structure is a good start, but we will extend it for clarity:
  - **Group Level:** Add `group_logic_type` (already inferred as “all_required”, “select_n_courses”, or “choose_one_section”) and potentially a numeric `required_sections` or similar if “select N sections” ever occurs (not common, but just in case). Also include the raw `instructions` text from ASSIST for traceability.
  - **Section Level:** Each section already has an `instructions` (which might be something like “A - All of the following:” or “B - Select 1 course from the following.”). We might parse that instruction to add fields like `select_count: 1` if applicable. E.g., Section B’s instruction “Select 1 course from the following” → `section_logic_type: "select_n_courses", required_count: 1`. This duplicates info but makes programmatic checking easier. If a section has no explicit instruction, default it to `section_logic_type: "all_required"` (meaning all courses listed in that section must be taken).
  - **Course Mapping Level:** Change from the current flat structure to a more detailed one. For example:
    ```json
    {
       "uc_course": {
          "code": "SOC 5",
          "title": "Sociology of Globalization and Social Change",
          "units": 4.00,
          "cross_listed": ["INTL 8"],
          "attributes": ["Must petition after transfer"]
       },
       "equivalents": [
          {
             "type": "OR",
             "courses": [
                {
                   "type": "Course",
                   "institution": "De Anza College",
                   "code": "SOCI 20",
                   "title": "Social Change in the Modern World",
                   "units": 4.00
                },
                {
                   "type": "Course",
                   "institution": "De Anza College",
                   "code": "SOCI 10",
                   "title": "American Society...",
                   "units": 4.00,
                   "no_articulation": true
                },
                ...
             ]
          }
       ]
    }
    ```
    This is more verbose, but it separates UC and CCC details and can include flags like `no_articulation` on a specific CCC course (as we know SOCI 10 had none). It also allows multiple OR groups if needed (though typically one OR-group per UC course).
    
    However, we can also keep a simplified approach: the current JSON has `equivalent_sets` which is either an OR list or a single AND, etc. We might just augment each course dict in those sets with a flag or property if it’s a placeholder (“No Course Articulated”) or if something like petition applies. In our code, we could detect the “Petition” note and simply attach a boolean `petition: true` to the `equivalent_sets` object or the course. For cross-listed on the UC side, maybe add `uc_course_note: "Same as INTL 8"`.
    
    Ultimately, the goal is to carry through any special conditions so they’re visible to the LLM. We might not need the LLM to read a deeply nested JSON if we pre-format the relevant info into a prompt in a readable way (like bullet points of what's required and what’s available).
    
  - **Major-Level Structure:** We might include meta like last updated timestamp (so the LLM can say “According to the 2024–2025 ASSIST agreement…” which adds credibility). Also a `source_url` if needed (though we might not expose that to user, but for our reference).

- **Prompt Construction Enhancements:**  
  With enriched JSON, we can construct prompts that are both **structured and readable**:
  - For example, if a user asks “What are the requirements for X major and how do I complete them at De Anza?”, we could generate a summary like:
    - *Group 1 (Core Coursework – all required):* UCSD Course A (requires De Anza equivalent X), UCSD Course B (equivalent Y or YH), UCSD Course C (no CC equivalent, must take at UCSD) ([Agreement Report](https://www.sdccd.edu/docs/accreditation/mesa/Evidence%20Collection%20Form/Standard%20II/Evidence%20Upload%20-%20Standard%20II%20-%20A10/II.A.10-06_Articulation%20Agreement%20by%20Major%20%20.pdf#:~:text=BENG%201%20,after%20transfer%20END%20OF%20AGREEMENT)), etc.
    - *Group 2 (Electives – choose 2):* Choose two from {UCSD D (De Anza M, N options), UCSD E (De Anza O option), UCSD F (no equivalent)}.
    - This summary can then be given to LLM to turn into prose.
    - We might even include the requirement logic in parentheses for clarity, like “(choose 2)” or “(all required)” so the LLM doesn’t have to infer it solely from text.
  
  - For direct questions about a course, the prompt might be as simple as a line from JSON turned into a sentence. E.g., *“UCSD MATH 20E has no De Anza equivalent. Note: Petition department after transfer. ([Agreement Report](https://www.sdccd.edu/docs/accreditation/mesa/Evidence%20Collection%20Form/Standard%20II/Evidence%20Upload%20-%20Standard%20II%20-%20A10/II.A.10-06_Articulation%20Agreement%20by%20Major%20%20.pdf#:~:text=Petition%20department%20after%20transfer%20%E2%86%90,Electricity))”*. The LLM will likely respond: “UCSD’s MATH 20E (Vector Calculus) doesn’t have an equivalent course at De Anza; you would have to take it at UCSD after you transfer, and possibly petition the department with any coursework you did elsewhere.” 
  
  - We will maintain all citations in the output when providing answers in the development phase (like the references to ASSIST lines we’ve used here) for verification. In production, we might not show raw citations, but the LLM can mention “(per ASSIST 2024–25 agreement)”.

- **Ensuring Consistency:**  
  As we scale to all majors, we want to ensure consistency in how information is presented. It might be useful to have the LLM follow a template when listing requirements (like always list the UC course and arrow to CC course, or always use phrases like “No course articulated” exactly when applicable). We can either instruct the LLM to use certain wording by priming it with examples, or we format the data in a quasi-natural way ourselves. For instance, we could feed:
  ```
  Requirements:
  1. MATH 18 (Linear Algebra) – **De Anza:** Math Diffeq 1 (example) – must be taken at UCSD (no equivalent).
  2. CSE 20 (Discrete Math) – **De Anza:** MATH 22 or 22H.
  3. ...
  ```
  And then ask the LLM to explain these to the student. The LLM would then have a consistent list to work off. However, it might be better for the LLM to do the listing to allow more fluid phrasing (it might incorporate them into sentences).
  
  For critical, repetitive phrases (“must be taken at the university after transfer,” “petition for credit”), we’ll ensure those appear in either the data or in our prompt so the LLM uses similar language to ASSIST (which is official and clear) ([Agreement Report](https://www.sdccd.edu/docs/accreditation/mesa/Evidence%20Collection%20Form/Standard%20II/Evidence%20Upload%20-%20Standard%20II%20-%20A10/II.A.10-06_Articulation%20Agreement%20by%20Major%20%20.pdf#:~:text=BENG%201%20,after%20transfer%20END%20OF%20AGREEMENT)) ([Agreement Report](https://www.sdccd.edu/docs/accreditation/mesa/Evidence%20Collection%20Form/Standard%20II/Evidence%20Upload%20-%20Standard%20II%20-%20A10/II.A.10-06_Articulation%20Agreement%20by%20Major%20%20.pdf#:~:text=Petition%20department%20after%20transfer%20%E2%86%90,Electricity)).

By iterating on the JSON design and prompt strategy now (in v2), we lay a strong foundation for scaling up, since any new data (other colleges, other UCs) will fit the same schema and the LLM prompts can remain largely the same.

## Best Practices from High-Trust Vertical LLMs  
In building TransferAI, we can draw from lessons in other domains where accuracy and trust are paramount, such as legal AI assistants, medical diagnosis LLMs, and tax advisors. Key takeaways applicable to our system include:

- **Grounding Responses in Authoritative Data:** Successful vertical LLM systems like legal assistants (e.g., Casetext’s CoCounsel or Harvey) always refer back to the law text or case data provided, rather than relying on the model’s memory. Similarly, TransferAI should treat the articulation JSON as the “law.” Our prompting ensures that the model uses the provided data as the basis for answers. For instance, if asked about a requirement, the model’s answer should map exactly to what ASSIST specifies, with no omissions or additions that aren’t in the data. If a student asks something like “Can I skip this requirement?”, the model should “consult” the rules (which would show no alternative unless explicitly stated) and answer accordingly. Keeping the model grounded prevents hallucination of equivalencies that don’t exist.

- **Structured Reasoning + Natural Language Synthesis:** In domains like tax or medicine, a common approach is to break a problem into structured steps (often handled by code or smaller model prompts) and then have the LLM articulate the conclusion in user-friendly language. We are adopting the same: our system will do structured checks (like counting courses completed vs required) and then prompt the LLM to explain it. This way, the **logical correctness** is ensured by structured methods, and the **empathetic, clear communication** is done by the LLM. This division of labor plays to the strengths of each component – much like a medical AI might calculate risk scores and then the LLM explains the diagnosis and next steps in simple terms.

- **Transparency and Explainability:** High-trust LLMs often provide explanations or cite sources so the user can verify the advice (e.g., an AI lawyer might cite the statute for a claim ([Receiving Articulation | ASSIST API Documentation](https://prod.assistng.org/apidocs/docs/articulation/model/receiving#:~:text=course%20,attributes%20associated%20with%20the%20course))). TransferAI can build trust by explaining the reasoning: not just *“Yes, you’re done”* but *“Yes, you completed X and Y which are the two required courses in that category, so you fulfilled that requirement.”* We can even quote the requirement instruction if needed. If something is unclear, the model can say *“According to ASSIST, no course at your college is equivalent to UCSD’s ABC 123, meaning you’ll have to take that at UCSD.”* Such statements give the student confidence that the answer is based on official policy, not just the AI’s guess. In interactive mode, if the student probes *“Why is there no equivalent?”*, the model can honestly say *“Some specialized courses have no community college version due to curriculum differences.”* This aligns with best practices: be truthful about what is known and unknown.

- **Managing Uncertainty:** In legal or medical fields, when the AI isn’t certain, it is often designed to either ask for clarification or advise consulting a human. TransferAI should do the same for ambiguous cases. For example, if a student asks about a course that isn’t clearly in the data (maybe a very new course or using a nickname), the AI can respond with something like *“I’m not finding that course in the articulation database. Is it possibly listed under a different name? You may need to check with an advisor to see how it transfers.”* It’s better to admit uncertainty than to guess and be wrong in a high-stakes scenario. We can include guidelines in the system prompt for this: e.g., “If unsure or data is not present, do not fabricate an answer – either ask for clarification or direct to official advising.”

- **Continuous Updates and Validation:** Vertical LLM solutions often have a human or expert in the loop for validation, especially in early stages. For TransferAI, while we aim to fully automate, we should incorporate feedback and perhaps a review mechanism. For v2, we (the developers) will test a lot of queries and compare answers to expected outcomes, refining prompts as needed. For v3 (broader launch), we might incorporate a way for users to flag an answer if it seems off, which we can later analyze. Also, because articulation agreements update annually, we will need to update our JSON data regularly (at least once per year). We should document a procedure for updating (maybe even automate scraping new catalog years) so the LLM always uses current information. Outdated info can severely hurt trust.

- **Balancing Strictness and Helpfulness:** Sometimes following the rules strictly vs. helping the user reach a solution can conflict. For example, strictly speaking, if a course isn’t articulated, the system should say it doesn’t count. But a helpful advisor might add: “...however, you might get it counted via petition” or “it could count for general education credit.” We should strive to provide those helpful extra insights (this is something a good human advisor would do). However, we must clearly distinguish official articulation from advice. We can phrase it as “Officially, no, it doesn’t count for the major. Unofficially, you could talk to the department about a possible substitution.” This approach is akin to medical AI providing options (“You’re not covered for that medication, but here are alternatives to discuss with your doctor.”). It gives the user agency and full information.

- **Privacy and Professional Tone:** In high-trust domains, the tone is usually formal yet empathetic. We should ensure TransferAI remains professional (e.g., using the student’s courses and context but not making jokes or side comments). Also, if user data is involved (like a list of courses they took), we handle it carefully. While not as sensitive as medical records, academic history is personal, so answers should respect privacy (which in a one-on-one system is fine) and be mindful (for instance, not disparaging a grade or course choice, just factual).

By following these best practices, we aim for TransferAI to be not just technically correct, but genuinely useful and trustworthy to students and counselors. As we scale up, these principles will guide development to maintain quality.

## Roadmap: From v2 to v3 and Beyond  
Finally, we outline the development roadmap to achieve the immediate goals for v2 and prepare for the ambitious expansion in v3.

### **v2: De Anza → UCSD (All Majors)**  
**Scope:** Cover every major offered at UC San Diego, articulating from De Anza College. This is roughly 150+ majors including all departments (Arts, Sciences, Engineering, etc.). Focus on lower-division major requirements as provided by ASSIST.

**Objectives and Tasks:**  
1. **Data Acquisition & Verification:** Use the improved scraper to collect articulation data for each major (for the 2024–2025 catalog year). Save each major’s JSON, possibly organized by department for sanity. Given the number of majors, automate this fully, but also implement checks:
   - After scraping, run a script to validate each JSON (e.g., ensure no group is empty unless intentionally, ensure all select_n instructions have the proper number, etc.). Cross-check a few random ones against the ASSIST website manually to ensure accuracy.
   - If any pattern fails (like a certain major’s page layout differs), adjust the parser accordingly and re-scrape that one.
   - Store these JSONs in a repository or database, and consider versioning (major and year).

2. **Reverse Index Building:** For convenience, build a lookup mapping from De Anza course to all UCSD majors/courses it applies to. This can be a simple Python dictionary persisted as JSON. This will let us answer reverse queries quickly in the app without scanning all majors. For v2, since it’s one community college and one campus, this is manageable (De Anza has a finite set of courses, and not all will appear in UCSD articulations; the index won’t be huge). This index can also serve to validate consistency (if the same De Anza course is listed under multiple majors’ agreements, it should map to the same UCSD course typically – if not, that’s interesting and we should be aware of it).

3. **Enhance JSON with Attributes:** Post-process the scraped JSONs to add flags/notes for “must take at UC”, “petition”, cross-list info, etc., where possible. This can be done by scanning instructions or course titles for known phrases. For example, if a UC course’s title or notes contain “petition” or “must be taken at the university”, attach a note. We saw these phrases in the Mesa College PDF, and likely the ASSIST interface includes them similarly. We might have to parse the text content of the `rowReceiving` cell for bullet symbols or known substrings. Do this systematically for all majors so none slip through. It’s okay if a few niche notes aren’t caught, but the common ones should be.

4. **LLM Prompt and Template Development:** Before deployment, develop a set of prompt templates for typical query types:
   - “What do I need to take for [requirement or major]?”
   - “Does [CCC course] fulfill [UC requirement]?”
   - “I took [list of courses] – which requirements are done/not done?”
   - “General questions (e.g., GPA, IGETC, etc.)”
   Create example dialogues for each and test with the LLM (likely GPT-4 or similar, given the complexity) using our data. Refine until the answers are correct and phrased well. These will essentially be the scenarios the system must handle.

5. **User Interface (if applicable for v2):** The prompt architecture implies certain interactions:
   - The user might first select or confirm their college (De Anza) and target (UCSD + major). In v1, perhaps this was fixed; in v2, if the user can choose different majors, we need a UI element or at least a conversation step. Possibly the user will tell the major in the question. We should prepare the system to identify the major (maybe use keyword matching or ask a follow-up for clarification if ambiguous).
   - Once context is set (De Anza -> UCSD, X Major), the system can answer multiple questions within that context easily. (We should carry that context so the user doesn’t have to repeat “for UCSD CS” every time.)
   - If building a chat interface, implement memory such that once the major is established, subsequent questions assume it, unless the user changes context.

6. **Testing & QA:** Thoroughly test TransferAI v2 with various majors:
   - We should simulate a student asking about each major’s requirements. This can be done via a script and the LLM API: feed it something like “Summarize what I need to take at De Anza for the ___ major at UCSD” for each major, and inspect the outputs. They should all make sense and not be empty or wrong. This will likely flush out any remaining parsing issues for odd majors.
   - Test tricky queries (partial info, wrong course codes, etc.) to see how the system handles them. Make sure it fails gracefully (e.g., “I’m sorry, I can’t find that course. Is it spelled correctly?” rather than giving nonsense).
   - Possibly involve a few actual counselors or transfer-savvy students to try it out and give feedback on the clarity and usefulness of answers.

7. **Launch and Documentation:** Deliver v2 with documentation on how it works and its current limitations (for example: “Advising covers lower-division major prep only, not general education or upper-division courses”). Encourage users to still consult a counselor for final verification, which is a good disclaimer to have.

**Features deferred to v3:** We might decide not to implement everything in v2 if time is short. For example, full handling of general education questions, or fancy UI features could wait. The core of v2 is covering all majors with correct articulation logic and having the LLM correctly utilize that data.

### **v3: All CCCs → All UCs**  
**Scope:** Expand TransferAI to support every California Community College and every UC campus – essentially any articulation on ASSIST. This is a significant jump in scale: there are 116 CCCs and 9 UCs (and thousands of majors across UCs). The data size and variety will grow enormously.

**Preparation and Considerations:**

- **Data Scaling:** Scraping or retrieving data for all CC–UC pairs is a large job. We should investigate the ASSIST API or data downloads. The ASSIST Next Gen site has an API (we saw references to “Get Articulation by Major” in their docs). If accessible, using the API could be more efficient and less brittle than scraping thousands of pages with Selenium. If API access is restricted, we may coordinate with ASSIST or use a caching strategy. Perhaps scrape a subset (like one UC at a time, or one CC at a time in batches) over time rather than all at once.
  
  At minimum, we should multi-thread or distribute the scraping to handle the volume. And definitely store results in a database. We may also only scrape on demand (when a user asks for a combination we haven’t stored), though that could introduce delay. Better is to pre-scrape popular combinations or at least all UCs for a given CCC and vice versa.

- **Knowledge Organization:** With all data, we’ll have tens of thousands of articulation mappings. We need a strategy to retrieve the relevant subset quickly for the LLM:
  - **Index by institution and major:** We can key data by `fromCollege, toUniversity, majorName`. When a user selects those (or asks about them), we load that specific JSON. This is similar to v2 but extended. We should design our data storage for quick lookup (maybe a simple key-value store with composite key, or a SQL DB).
  - **Vector Search for Q&A:** We might create an embedding of each articulation line or requirement, enabling semantic search. For example, if a user asks “Does College X’s course Y count for anything in UCZ?”, we could do a semantic search across all articulation data for that course. However, this might be overkill if our structured search is accurate. But semantic search could handle misspellings or indirect questions better. This could be an enhancement if needed.
  
- **Generalization of Patterns:** By v3, we will have seen the full diversity of articulation logic. There might be new patterns from other UCs (perhaps some UCs have unique requirements or wording). We should continuously update our parser logic to handle any new phrases. For example, one UC might say “choose one emphasis” or something unusual. We should incorporate those as we discover them. The good news is the ASSIST platform standardizes a lot, so it’s likely similar across campuses, but we must be vigilant.

- **Major Naming Variants:** When dealing with all CCC and all UCs, be mindful that major names might need normalization. For instance, one could ask “computer science at UCLA” or “CS major at UCLA”. We should have a mapping or fuzzy matching to the official major name as listed on ASSIST (which might be “Computer Science B.S.”). Perhaps maintain a list of majors for each campus and their common abbreviations to interpret user queries. This is more NLP, but necessary for a good UX.

- **Multi-College Comparison or Planning:** In v3+, a feature might be to compare pathways (e.g., what if I go to College A vs College B for transferring to UC?). Or a student might attend multiple community colleges. If it comes up, our system could handle multiple CCC sources by combining data. This complicates logic (ensuring the courses across both schools fulfill the requirements without overlap issues). This is an edge case that could be tackled in v3 or v4. For now, assume one CCC at a time.

- **Integration with Counseling Workflows:** For widespread adoption, consider how TransferAI fits in. Possibly, provide a frontend where students select their college and target campus/major, then ask questions. Or integrate into college websites. This might require a simple web UI or even integration into existing tools. While technical, it’s part of roadmap to think of packaging this into a service that can be easily used. 

- **Quality Assurance at Scale:** With so many combinations, we’ll rely on automated tests and possibly user feedback. We could implement a logging and analytics system to see which questions get asked and how the AI responds, then flag any incorrect answers for review. Periodic retraining or prompt tuning might be needed if we see patterns of confusion.

- **Extending to New Domains:** After covering major requirements, a logical next step could be to incorporate **GE (IGETC) advising** or **CSU transfer articulation**, since ASSIST also covers CSU. The system architecture would support it (just more data and possibly different rules like IGETC categories). This could be v4. However, we must be cautious not to overextend until the UC major advising is rock-solid.

- **Maintaining High Trust:** As we scale, we should avoid letting the quality slip. This means continuing to use the strategies from v2 for each new piece of data. Also, being transparent that v3 covers many schools but the student should ensure they selected the correct ones (since an answer for De Anza→UCSD might differ for De Anza→UCLA). Possibly implement a double-check in the conversation: “You’re asking about transferring from X to Y, correct?” to avoid any mis-context.

- **Performance Considerations:** More data could mean slower responses if not handled well. We might need to cache LLM outputs for common questions (though each student scenario is unique). Or if using an API model, ensure we have enough throughput. If volume grows, possibly fine-tune a smaller LLM on our Q&A pairs to reduce cost – that’s an exploration for future (vertical-specific fine-tunes can work well for known formats, but GPT-4’s reasoning might still be superior in tricky cases).

In summary, the roadmap to v3 is about **scale and breadth** – scaling the data coverage and ensuring our architecture (scraping, data storage, retrieval, prompting) can handle that scale without losing accuracy. Each new addition (a college, a campus) should ideally be a configuration change, not a fundamental code change, thanks to the generalizations we built in v2.

## Conclusion  
By implementing the above roadmap, TransferAI will evolve from a single-scenario prototype into a comprehensive advising system for California transfers. In **v2**, we focus on mastering the articulation logic for one college-to-campus pairing across all majors, refining our data parsing and prompt strategy to handle every quirk (all-required vs optional, cross-listed, petitions, etc.) ([2023-2024 General Education Requirements Agreement - ASSIST](https://www.assist.org/transfer/support?year=74&institution=132&agreement=114&agreementType=from&view=agreement&viewBy=breadth&viewSendingAgreements=false&viewByKey=74%2F114%2Fto%2F132%2FGeneralEducation%2F21ec3bad-8a12-407a-b843-89a413ee0729#:~:text=Students%20with%20partial%20IGETC%20certification,J)) ([Agreement Report](https://www.sdccd.edu/docs/accreditation/mesa/Evidence%20Collection%20Form/Standard%20II/Evidence%20Upload%20-%20Standard%20II%20-%20A10/II.A.10-06_Articulation%20Agreement%20by%20Major%20%20.pdf#:~:text=BENG%201%20,after%20transfer%20END%20OF%20AGREEMENT)). We also make the LLM smarter with layered prompts and integrated rules, so it can answer both straightforward and complex questions with confidence and clarity. 

Armed with that foundation, we can then tackle **v3: scaling up** to the full statewide articulation network. By adhering to best practices from other vertical AI solutions – particularly maintaining strict fidelity to authoritative data while providing user-friendly explanations – we ensure that TransferAI remains a **trusted assistant** for students. It will empower them to navigate the transfer maze, knowing exactly which courses to take and how their community college work will pay off at the UC. 

Through careful engineering and a strong roadmap, TransferAI can significantly streamline the transfer planning process, making higher education pathways more transparent and accessible. The steps outlined here give a clear path to follow, from deepening our handling of articulation logic to broadening our coverage to every student and every campus in California. With v2’s success, we’ll be well on our way to a truly comprehensive transfer advising AI. 

